# Relation-Net-with-Stack-Attention

Combine https://arxiv.org/abs/1706.01427 with https://arxiv.org/abs/1511.02274 to explore whether attention mechanism brings hints to reduce relation net complexity. 

# For now, only use two attention layers.

TODO_1: Apply weights to Element-wise sum between two MLPs.

TODO_2: Apply weights before the first MLP to "dropout" some relations.
